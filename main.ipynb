{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch\n",
    "# !pip install transformers\n",
    "# !pip install easydict\n",
    "# !pip install colab-ssh --upgrade\n",
    "# !pip install openpyxl\n",
    "\n",
    "# from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
    "# launch_ssh_cloudflared(password='0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # mount Google Drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# GDRIVE_HOME = '/content/drive/MyDrive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments to do\n",
    "21-05-17 12:29am\n",
    "1. bert_attscore_bi_rnn\n",
    "2. bert_attscore_rnn_add_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Experiment Option\n",
    "from easydict import EasyDict\n",
    "import torch\n",
    "\n",
    "opt = EasyDict()\n",
    "opt.dataset_series = 'SemEval-16' # SemEval-16, sentihood\n",
    "opt.dataset_domain = 'restaurant' # restaurant / laptop\n",
    "opt.subtask = 'sub1' # sub1: sentence, sub2: document(full review)\n",
    "opt.task = 'category' # category, term\n",
    "opt.num_classes = 3 # negative, positive, neutral, (+ conflict)\n",
    "opt.max_length = 200\n",
    "opt.model_name = 'bert_attscore_forcls_rnn'\n",
    "# model_name: {bert_base, bert_attscore, bert_attscore_rnn, bert_attscore_bi_rnn, bert_attscore_rnn_add_asp,\n",
    "#    bert_attscore_rnn_add_sep1, bert_attscore_rnn_add_sep_both, bert_attscore_forcls_rnn}\n",
    "opt.pos = False\n",
    "opt.lastid = False\n",
    "opt.top_k = 3\n",
    "opt.valset_ratio = 0.2\n",
    "opt.batch_size = 16\n",
    "opt.num_layers = 6\n",
    "opt.num_epochs = 12\n",
    "opt.runs = 5\n",
    "opt.seed = 42\n",
    "opt.log_step = 100\n",
    "opt.patience = 5\n",
    "opt.device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "print(opt.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     4,
     7
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set: 2,507\n",
      "length of test set: 859\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# research_root = os.path.join(GDRIVE_HOME, 'research')\n",
    "# sys.path.append(research_root)\n",
    "\n",
    "if opt.dataset_series == 'SemEval-16':\n",
    "    path = 'dataset/{}/semeval16_{}_{}.csv'.format(opt.dataset_series, opt.subtask, opt.dataset_domain)\n",
    "    path_test = 'dataset/{}/semeval16_{}_{}_test.csv'.format(opt.dataset_series, opt.subtask, opt.dataset_domain)\n",
    "elif opt.dataset_series == 'sentihood':\n",
    "    path = 'dataset/{}/sentihood_train.csv'.format(opt.dataset_series)\n",
    "    path_test = 'dataset/{}/sentihood_test.csv'.format(opt.dataset_series)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(path)\n",
    "df_test = pd.read_csv(path_test)\n",
    "\n",
    "print('length of train set: {:,}'.format(len(df_train)))\n",
    "print('length of test set: {:,}'.format(len(df_test)))\n",
    "\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'attscore' in opt.model_name:\n",
    "    from data_utils import clean_sentence, preprocess\n",
    "    df_train = clean_sentence(df=df_train, clean_func=preprocess)\n",
    "    df_test = clean_sentence(df=df_test, clean_func=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentihood: False\n",
      "2,507 samples in this dataset\n",
      "sentihood: False\n",
      "859 samples in this dataset\n"
     ]
    }
   ],
   "source": [
    "from data_utils import Category_Classification_Dataset as Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "trainset = Dataset(df=df_train, tokenizer=tokenizer, opt=opt, pos_encoding=False)\n",
    "testset = Dataset(df=df_test, tokenizer=tokenizer, opt=opt, pos_encoding=False)\n",
    "\n",
    "# print(trainset.get_sample(423))\n",
    "# print('-'*30)\n",
    "# print(trainset[423])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of datasets 2006 : 501 : 859\n"
     ]
    }
   ],
   "source": [
    "from data_utils import custom_random_split as rs\n",
    "\n",
    "train_set, val_set, test_set = rs(dataset=trainset, testset=testset,\n",
    "                                  val_ratio=opt.valset_ratio, random_seed=opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=opt.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=opt.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[  101,  1996,  3295,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2022,  2469,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  1996, 24857,  ...,     0,     0,     0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  101,  1996, 14163,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2326,  2003,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2204, 20861,  ...,     0,     0,     0]]]),\n",
       " 'attention_masks': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]]]),\n",
       " 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]]]),\n",
       " 'labels': tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = iter(train_loader).next()\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pair 단어들 (102번 사이)과 첫 문장 단어들 간의 attention score 합을 기준으로 top-k개 단어 선별\n",
    "    - 그 단어들의 mean pool\n",
    "    - 그 단어들을 rnn layer에?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_attscore_forcls_rnn'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from models.bert_intermediate import *\n",
    "from models.bert_pos import *\n",
    "from models.bert_attscores import *\n",
    "\n",
    "if opt.model_name == 'bert_base':\n",
    "    model = Bert_Base(opt.num_classes)\n",
    "elif opt.model_name == 'bert_attscore':\n",
    "    model = Bert_AttScore(opt=opt, embed_dim=768, fc_hid_dim=128, top_k=3, att_head='all', att_pooling='mean')\n",
    "elif opt.model_name == 'bert_attscore_rnn':\n",
    "    model = Bert_AttScore_RNN(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=False,\n",
    "                              top_k=opt.top_k, att_head='all', att_pooling='gru')\n",
    "elif opt.model_name == 'bert_attscore_bi_rnn':\n",
    "    model = Bert_AttScore_RNN(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=True,\n",
    "                              top_k=opt.top_k, att_head='all', att_pooling='gru')\n",
    "elif opt.model_name == 'bert_attscore_rnn_add_sep1':\n",
    "    model = Bert_AttScore_RNN_add(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=True,\n",
    "                                 top_k=opt.top_k, att_head='all', additional_token='sep1', att_pooling='gru')\n",
    "elif opt.model_name == 'bert_attscore_rnn_add_sep2':\n",
    "    model = Bert_AttScore_RNN_add(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=True,\n",
    "                                 top_k=opt.top_k, att_head='all', additional_token='sep2', att_pooling='gru')\n",
    "elif opt.model_name == 'bert_attscore_rnn_add_sep_both':\n",
    "    model = Bert_AttScore_RNN_add(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=True,\n",
    "                                 top_k=opt.top_k, att_head='all', additional_token='sep_both', att_pooling='gru')\n",
    "elif opt.model_name == 'bert_attscore_rnn_add_asp':\n",
    "    model = Bert_AttScore_RNN_add(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=True,\n",
    "                                 top_k=opt.top_k, att_head='all', additional_token='asp', att_pooling='gru')\n",
    "elif opt.model_name == 'bert_attscore_rnn_add_cls':\n",
    "    model = Bert_AttScore_RNN_add(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=True,\n",
    "                                 top_k=opt.top_k, att_head='all', additional_token='cls', att_pooling='gru')\n",
    "elif opt.model_name == 'bert_attscore_forcls_rnn':\n",
    "    model = Bert_AttScore_forCLS_RNN(opt=opt, embed_dim=768, rnn_hid_dim=256, fc_hid_dim=128, bidirectional=True,\n",
    "                                 top_k=opt.top_k, att_head='all', att_pooling='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111,059,715 total parameters in this model\n",
      "111,059,715 trainable parameters in this model\n"
     ]
    }
   ],
   "source": [
    "from models.parameters import get_parameters\n",
    "total, params = get_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some tokens in vocab\n",
    "# emb_layer = nn.Embedding(30524, 768, padding_idx=0)\n",
    "# torch.nn.init.normal_(emb_layer.weight)\n",
    "# model.bert.bert.embeddings.word_embeddings = emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> RUN NUMBER: 01 <<<<<\n",
      "   global step: 100 | train loss: 0.557, train_acc: 78.12%\n",
      "Epoch: 01 | Val Loss: 0.031 | Val Acc: 82.44%\n",
      "   global step: 200 | train loss: 0.272, train_acc: 90.71%\n",
      "Epoch: 02 | Val Loss: 0.031 | Val Acc: 81.84%\n",
      "   global step: 300 | train loss: 0.197, train_acc: 92.71%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_3_val_acc_79.44%\n",
      "Epoch: 03 | Val Loss: 0.033 | Val Acc: 79.44%\n",
      "   global step: 400 | train loss: 0.145, train_acc: 96.02%\n",
      "   global step: 500 | train loss: 0.171, train_acc: 93.75%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_4_val_acc_81.44%\n",
      "Epoch: 04 | Val Loss: 0.039 | Val Acc: 81.44%\n",
      "   global step: 600 | train loss: 0.122, train_acc: 95.64%\n",
      "Epoch: 05 | Val Loss: 0.041 | Val Acc: 80.04%\n",
      "   global step: 700 | train loss: 0.106, train_acc: 95.27%\n",
      "Epoch: 06 | Val Loss: 0.046 | Val Acc: 80.84%\n",
      "   global step: 800 | train loss: 0.073, train_acc: 96.88%\n",
      "Epoch: 07 | Val Loss: 0.048 | Val Acc: 80.44%\n",
      "   global step: 900 | train loss: 0.073, train_acc: 97.92%\n",
      "   global step: 1,000 | train loss: 0.078, train_acc: 96.66%\n",
      "Epoch: 08 | Val Loss: 0.047 | Val Acc: 81.04%\n",
      "   global step: 1,100 | train loss: 0.066, train_acc: 97.08%\n",
      ">> early stop\n",
      "Best Val Acc: 81.44% at 4 epoch\n",
      ">> saved best state dict: state_dict/BEST_bert_attscore_forcls_rnn_SemEval-16_restaurant_val_acc_81.44%\n",
      "RUN: 01 | Test loss: 0.032 | Test_acc: 87.31% | Test_f1: 66.62\n",
      ">>>>> RUN 01 HAS BEEN FINISHED <<<<<\n",
      ">>>>> RUN NUMBER: 02 <<<<<\n",
      "   global step: 100 | train loss: 0.590, train_acc: 74.31%\n",
      "Epoch: 01 | Val Loss: 0.034 | Val Acc: 81.04%\n",
      "   global step: 200 | train loss: 0.305, train_acc: 89.27%\n",
      "Epoch: 02 | Val Loss: 0.037 | Val Acc: 81.64%\n",
      "   global step: 300 | train loss: 0.194, train_acc: 93.10%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_3_val_acc_81.24%\n",
      "Epoch: 03 | Val Loss: 0.034 | Val Acc: 81.24%\n",
      "   global step: 400 | train loss: 0.174, train_acc: 94.89%\n",
      "   global step: 500 | train loss: 0.168, train_acc: 93.90%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_4_val_acc_81.84%\n",
      "Epoch: 04 | Val Loss: 0.039 | Val Acc: 81.84%\n",
      "   global step: 600 | train loss: 0.118, train_acc: 95.57%\n",
      "Epoch: 05 | Val Loss: 0.042 | Val Acc: 79.84%\n",
      "   global step: 700 | train loss: 0.097, train_acc: 95.98%\n",
      "Epoch: 06 | Val Loss: 0.050 | Val Acc: 81.24%\n",
      "   global step: 800 | train loss: 0.080, train_acc: 97.16%\n",
      "Epoch: 07 | Val Loss: 0.045 | Val Acc: 81.04%\n",
      "   global step: 900 | train loss: 0.071, train_acc: 96.88%\n",
      "   global step: 1,000 | train loss: 0.084, train_acc: 96.19%\n",
      "Epoch: 08 | Val Loss: 0.054 | Val Acc: 81.24%\n",
      "   global step: 1,100 | train loss: 0.083, train_acc: 95.86%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_9_val_acc_82.63%\n",
      "Epoch: 09 | Val Loss: 0.053 | Val Acc: 82.63%\n",
      "   global step: 1,200 | train loss: 0.052, train_acc: 97.44%\n",
      "Epoch: 10 | Val Loss: 0.064 | Val Acc: 81.04%\n",
      "   global step: 1,300 | train loss: 0.055, train_acc: 97.66%\n",
      "Epoch: 11 | Val Loss: 0.056 | Val Acc: 82.44%\n",
      "   global step: 1,400 | train loss: 0.022, train_acc: 99.11%\n",
      "   global step: 1,500 | train loss: 0.056, train_acc: 97.15%\n",
      "Epoch: 12 | Val Loss: 0.060 | Val Acc: 81.24%\n",
      "Best Val Acc: 82.63% at 9 epoch\n",
      ">> saved best state dict: state_dict/BEST_bert_attscore_forcls_rnn_SemEval-16_restaurant_val_acc_82.63%\n",
      "RUN: 02 | Test loss: 0.036 | Test_acc: 87.78% | Test_f1: 71.06\n",
      ">>>>> RUN 02 HAS BEEN FINISHED <<<<<\n",
      ">>>>> RUN NUMBER: 03 <<<<<\n",
      "   global step: 100 | train loss: 0.594, train_acc: 75.62%\n",
      "Epoch: 01 | Val Loss: 0.031 | Val Acc: 81.64%\n",
      "   global step: 200 | train loss: 0.310, train_acc: 87.84%\n",
      "Epoch: 02 | Val Loss: 0.032 | Val Acc: 82.44%\n",
      "   global step: 300 | train loss: 0.207, train_acc: 92.97%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_3_val_acc_82.44%\n",
      "Epoch: 03 | Val Loss: 0.033 | Val Acc: 82.44%\n",
      "   global step: 400 | train loss: 0.117, train_acc: 95.74%\n",
      "   global step: 500 | train loss: 0.146, train_acc: 94.62%\n",
      "Epoch: 04 | Val Loss: 0.037 | Val Acc: 81.84%\n",
      "   global step: 600 | train loss: 0.111, train_acc: 96.03%\n",
      "Epoch: 05 | Val Loss: 0.041 | Val Acc: 81.44%\n",
      "   global step: 700 | train loss: 0.085, train_acc: 96.70%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_6_val_acc_83.23%\n",
      "Epoch: 06 | Val Loss: 0.037 | Val Acc: 83.23%\n",
      "   global step: 800 | train loss: 0.067, train_acc: 97.30%\n",
      "Epoch: 07 | Val Loss: 0.041 | Val Acc: 82.24%\n",
      "   global step: 900 | train loss: 0.062, train_acc: 97.57%\n",
      "   global step: 1,000 | train loss: 0.074, train_acc: 96.77%\n",
      "Epoch: 08 | Val Loss: 0.043 | Val Acc: 83.23%\n",
      "   global step: 1,100 | train loss: 0.064, train_acc: 96.81%\n",
      "Epoch: 09 | Val Loss: 0.047 | Val Acc: 82.04%\n",
      "   global step: 1,200 | train loss: 0.046, train_acc: 97.92%\n",
      "Epoch: 10 | Val Loss: 0.046 | Val Acc: 82.24%\n",
      "   global step: 1,300 | train loss: 0.043, train_acc: 97.81%\n",
      ">> early stop\n",
      "Best Val Acc: 83.23% at 6 epoch\n",
      ">> saved best state dict: state_dict/BEST_bert_attscore_forcls_rnn_SemEval-16_restaurant_val_acc_83.23%\n",
      "RUN: 03 | Test loss: 0.037 | Test_acc: 87.43% | Test_f1: 69.48\n",
      ">>>>> RUN 03 HAS BEEN FINISHED <<<<<\n",
      ">>>>> RUN NUMBER: 04 <<<<<\n",
      "   global step: 100 | train loss: 0.585, train_acc: 74.75%\n",
      "Epoch: 01 | Val Loss: 0.033 | Val Acc: 81.44%\n",
      "   global step: 200 | train loss: 0.330, train_acc: 88.01%\n",
      "Epoch: 02 | Val Loss: 0.031 | Val Acc: 81.84%\n",
      "   global step: 300 | train loss: 0.232, train_acc: 91.93%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_3_val_acc_81.24%\n",
      "Epoch: 03 | Val Loss: 0.037 | Val Acc: 81.24%\n",
      "   global step: 400 | train loss: 0.180, train_acc: 94.32%\n",
      "   global step: 500 | train loss: 0.168, train_acc: 93.95%\n",
      "Epoch: 04 | Val Loss: 0.036 | Val Acc: 81.04%\n",
      "   global step: 600 | train loss: 0.124, train_acc: 95.12%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_5_val_acc_82.44%\n",
      "Epoch: 05 | Val Loss: 0.040 | Val Acc: 82.44%\n",
      "   global step: 700 | train loss: 0.092, train_acc: 96.25%\n",
      "Epoch: 06 | Val Loss: 0.044 | Val Acc: 81.24%\n",
      "   global step: 800 | train loss: 0.081, train_acc: 96.59%\n",
      "Epoch: 07 | Val Loss: 0.045 | Val Acc: 81.44%\n",
      "   global step: 900 | train loss: 0.070, train_acc: 97.22%\n",
      "   global step: 200 | train loss: 0.340, train_acc: 88.18%\n",
      "Epoch: 02 | Val Loss: 0.033 | Val Acc: 81.84%\n",
      "   global step: 300 | train loss: 0.219, train_acc: 91.54%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_3_val_acc_81.44%\n",
      "Epoch: 03 | Val Loss: 0.034 | Val Acc: 81.44%\n",
      "   global step: 400 | train loss: 0.150, train_acc: 95.74%\n",
      "   global step: 500 | train loss: 0.157, train_acc: 94.21%\n",
      ">> saved: state_dict/bert_attscore_forcls_rnn_SemEval-16_restaurant_epoch_4_val_acc_81.64%\n",
      "Epoch: 04 | Val Loss: 0.039 | Val Acc: 81.64%\n",
      "   global step: 600 | train loss: 0.120, train_acc: 95.64%\n",
      "Epoch: 05 | Val Loss: 0.040 | Val Acc: 80.64%\n",
      "   global step: 700 | train loss: 0.095, train_acc: 96.43%\n",
      "Epoch: 06 | Val Loss: 0.040 | Val Acc: 81.24%\n",
      "   global step: 800 | train loss: 0.092, train_acc: 96.59%\n",
      "Epoch: 07 | Val Loss: 0.049 | Val Acc: 80.84%\n",
      "   global step: 900 | train loss: 0.075, train_acc: 96.88%\n",
      "   global step: 1,000 | train loss: 0.081, train_acc: 96.08%\n",
      "Epoch: 08 | Val Loss: 0.048 | Val Acc: 81.04%\n",
      "   global step: 1,100 | train loss: 0.060, train_acc: 97.15%\n",
      ">> early stop\n",
      "Best Val Acc: 81.64% at 4 epoch\n",
      ">> saved best state dict: state_dict/BEST_bert_attscore_forcls_rnn_SemEval-16_restaurant_val_acc_81.64%\n",
      "RUN: 05 | Test loss: 0.029 | Test_acc: 88.59% | Test_f1: 70.00\n",
      ">>>>> RUN 05 HAS BEEN FINISHED <<<<<\n",
      "Sheets has been exported\n",
      "====================\n",
      "best run: 05 | best acc: 88.59 | best f1: 70.00 | best run path: state_dict/BEST_bert_attscore_forcls_rnn_SemEval-16_restaurant_val_acc_81.64%\n",
      "<<<Averages>>>\n",
      "test_loss: 0.034 | test_acc: 87.52 | test_f1: 69.36\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from custom_trainer import *\n",
    "\n",
    "optimizer = optim.AdamW(params, lr=2e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8) # can't use for multiple runs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "result_dict, best_path = runs(trainer=trainer, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "                             model=model, criterion=criterion, optimizer=optimizer, scheduler=False, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
