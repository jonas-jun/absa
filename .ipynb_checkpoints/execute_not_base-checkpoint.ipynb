{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e183df58",
   "metadata": {},
   "source": [
    "#### for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0cd18",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install colab-ssh --upgrade\n",
    "\n",
    "# from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
    "# launch_ssh_cloudflared(password='0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b8425",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # mount Google Drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# GDRIVE_HOME = '/content/drive/MyDrive'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfcf2f",
   "metadata": {},
   "source": [
    "## Experiment options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88410a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "import torch\n",
    "\n",
    "opt = EasyDict()\n",
    "opt.dataset_series = 'SemEval-16' # SemEval-16, sentihood\n",
    "opt.dataset_domain = 'restaurant' # restaurant / laptop\n",
    "opt.subtask = 'sub1' # sub1: sentence, sub2: document(full review)\n",
    "opt.num_classes = 3 # negative, positive, neutral, (+ conflict)\n",
    "opt.max_length = 200\n",
    "opt.model_name = 'bert_high_attention_top_k_lastid_rnn' # bert_intermediate_base / bert_intermediate_att\n",
    "opt.pos = True\n",
    "opt.lastid = True\n",
    "opt.valset_ratio = 0.2\n",
    "opt.batch_size = 16\n",
    "opt.num_layers = 6 # bert intermediate\n",
    "opt.num_epochs = 10\n",
    "opt.seed = 42\n",
    "opt.log_step = 100\n",
    "opt.patience = 5\n",
    "opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(opt.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a4c15",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fddcac9",
   "metadata": {
    "code_folding": [
     4,
     7
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set: 2,507\n",
      "length of test set: 859\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>re_idx</th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>category</th>\n",
       "      <th>polarity</th>\n",
       "      <th>target</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1004293:0</td>\n",
       "      <td>Judging from previous posts this used to be a ...</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "      <td>place</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1004293:1</td>\n",
       "      <td>We, there were four of us, arrived at noon - t...</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "      <td>staff</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1004293:2</td>\n",
       "      <td>They never brought us complimentary noodles, i...</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1004293:3</td>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>FOOD#QUALITY</td>\n",
       "      <td>negative</td>\n",
       "      <td>food</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1004293:3</td>\n",
       "      <td>The food was lousy - too sweet or too salty an...</td>\n",
       "      <td>FOOD#STYLE_OPTIONS</td>\n",
       "      <td>negative</td>\n",
       "      <td>portions</td>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   re_idx        idx                                           sentence  \\\n",
       "0       0  1004293:0  Judging from previous posts this used to be a ...   \n",
       "1       1  1004293:1  We, there were four of us, arrived at noon - t...   \n",
       "2       2  1004293:2  They never brought us complimentary noodles, i...   \n",
       "3       3  1004293:3  The food was lousy - too sweet or too salty an...   \n",
       "4       3  1004293:3  The food was lousy - too sweet or too salty an...   \n",
       "\n",
       "             category  polarity    target  from  to  \n",
       "0  RESTAURANT#GENERAL  negative     place    51  56  \n",
       "1     SERVICE#GENERAL  negative     staff    75  80  \n",
       "2     SERVICE#GENERAL  negative       NaN     0   0  \n",
       "3        FOOD#QUALITY  negative      food     4   8  \n",
       "4  FOOD#STYLE_OPTIONS  negative  portions    52  60  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "# research_root = os.path.join(GDRIVE_HOME, 'research')\n",
    "# sys.path.append(research_root)\n",
    "\n",
    "if opt.dataset_series == 'SemEval-16':\n",
    "    path = 'dataset/{}/semeval16_{}_{}.csv'.format(opt.dataset_series, opt.subtask, opt.dataset_domain)\n",
    "    path_test = 'dataset/{}/semeval16_{}_{}_test.csv'.format(opt.dataset_series, opt.subtask, opt.dataset_domain)\n",
    "elif opt.dataset_series == 'sentihood':\n",
    "    path = 'dataset/{}/sentihood_train.csv'.format(opt.dataset_series)\n",
    "    path_test = 'dataset/{}/sentihood_test.csv'.format(opt.dataset_series)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(path)\n",
    "df_test = pd.read_csv(path_test)\n",
    "\n",
    "print('length of train set: {:,}'.format(len(df_train)))\n",
    "print('length of test set: {:,}'.format(len(df_test)))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a797ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431c18d35ae8476d9f654d2b744dcc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e941598b98924293ab961f10bb67714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9a7d33c00e43da92bb822d01c59608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,507 samples in this dataset\n",
      "859 samples in this dataset\n"
     ]
    }
   ],
   "source": [
    "from data_utils import Category_Classification_Dataset as Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "trainset = Dataset(df=df_train, tokenizer=tokenizer, max_length=opt.max_length, pos_encoding=False)\n",
    "testset = Dataset(df=df_test, tokenizer=tokenizer, max_length=opt.max_length, pos_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3d3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I have reservations about the all you can eat deal, however -- the choices are fairly limited and you can probably order more food than you can eat for less than $18 by just going off the menu.\n",
      "Aspect Category: FOOD#STYLE_OPTIONS\n",
      "Polarity: negative\n",
      "Input IDs: tensor([[  101,  1045,  2031, 17829,  2055,  1996,  2035,  2017,  2064,  4521,\n",
      "          3066,  1010,  2174,  1011,  1011,  1996,  9804,  2024,  7199,  3132,\n",
      "          1998,  2017,  2064,  2763,  2344,  2062,  2833,  2084,  2017,  2064,\n",
      "          4521,  2005,  2625,  2084,  1002,  2324,  2011,  2074,  2183,  2125,\n",
      "          1996, 12183,  1012,   102,  2833,  2806,  1035,  7047,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "Token type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Encoded Label: 0\n",
      "None\n",
      "------------------------------\n",
      "{'input_ids': tensor([[  101,  1045,  2031, 17829,  2055,  1996,  2035,  2017,  2064,  4521,\n",
      "          3066,  1010,  2174,  1011,  1011,  1996,  9804,  2024,  7199,  3132,\n",
      "          1998,  2017,  2064,  2763,  2344,  2062,  2833,  2084,  2017,  2064,\n",
      "          4521,  2005,  2625,  2084,  1002,  2324,  2011,  2074,  2183,  2125,\n",
      "          1996, 12183,  1012,   102,  2833,  2806,  1035,  7047,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_masks': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "print(trainset.get_sample(423))\n",
    "print('-'*30)\n",
    "print(trainset[423])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "536415f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of datasets: 2006 : 501 : 859\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Stable Random Seed\n",
    "SEED = opt.seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True # ?\n",
    "torch.backends.cudnn.benchmark = False # ?\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "val_ratio = opt.valset_ratio\n",
    "num_val = int(len(trainset) * val_ratio)\n",
    "num_train = len(trainset) - num_val\n",
    "train_set, val_set = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(SEED))\n",
    "#train_asp_idx, val_asp_idx = random_split(train_asp_idxs, [num_train, num_val], generator=torch.Generator().manual_seed(SEED)) # get aspect index\n",
    "test_set = testset\n",
    "\n",
    "print('Ratio of datasets: {} : {} : {}'.format(len(train_set), len(val_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae08c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=opt.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=opt.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "614a9f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[  101,  1996,  3295,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2022,  2469,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  1996, 24857,  ...,     0,     0,     0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  101,  1996, 14163,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2326,  2003,  ...,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2204, 20861,  ...,     0,     0,     0]]]),\n",
       " 'attention_masks': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]]]),\n",
       " 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0]]]),\n",
       " 'labels': tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = iter(train_loader).next()\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee16d5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d5ab4",
   "metadata": {},
   "source": [
    "1. pair 단어들 (102번 사이)과 첫 문장 단어들 간의 attention score 합을 기준으로 top-k개 단어 선별\n",
    "    - 그 단어들의 mean pool\n",
    "    - 그 단어들을 rnn layer에?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "\n",
    "class Bert_Att_Scores(nn.Module):\n",
    "    def __init__(self, opt, embed_dim=768, fc_hidden_dim=128, top_k=3, att_pooling='mean'):\n",
    "        super(Bert_Att_Scores, self).__init__()\n",
    "        self.num_classes = opt.num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.fc_hidden_dim = fc_hidden_dim\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.top_k = top_k\n",
    "        self.att_pooling = att_pooling\n",
    "        if att_pooling == 'concat':\n",
    "            self.fc1 = nn.Linear((self.embed_dim)*top_k, self.num_classes)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(self.embed_dim, self.num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        #self.fc2 = nn.Linear(self.fc_hidden_dim, self.num_classes)\n",
    "        self.device = opt.device\n",
    "\n",
    "    def forward(self, input_ids, att_mask, token_ids, pos_ids, last_ids):\n",
    "\n",
    "        asp_ids = list()\n",
    "        for i in pos_ids:\n",
    "            ids = (pos_ids==0).nonzero(as_tuple=True)[0].tolist()\n",
    "            asp_ids.append(ids)\n",
    "\n",
    "        output_dict = self.bert(input_ids, attention_mask=att_mask, token_type_ids=token_ids,\n",
    "                output_attentions=True, encoder_hidden_states=True, return_dict=True)\n",
    "        \n",
    "        # get top-k att idx in final att layer\n",
    "        atts = output_dict.attentions[-1]\n",
    "        in_batch_atts = list()\n",
    "        for a in atts:\n",
    "            in_batch_atts.append(sum(a) / a.size(0)) # average of all att. heads, each (8, 200, 200)\n",
    "        top_k_idx = list()\n",
    "        for att, asp, last in zip(in_batch_atts, asp_ids, last_ids):\n",
    "            sum_ = sum(att[asp[0]:(asp[-1]+1), :]) # sum attention scores for multi-aspect words (1, 200)\n",
    "            idxs = torch.sort(sum_[1:last+1], descending=True).indices[:self.top_k] + 1 # exclude 0(<CLS>), last(<SEP>)\n",
    "            top_k_idx.append(idxs) # re sum 1 to include 0(<CLS>)\n",
    "        # len(top_k_idx): batch_size\n",
    "        # top_k_idx[0].shape = [1,3]\n",
    "\n",
    "        # get top-k hidden states\n",
    "        hids = output_dict.last_hidden_state\n",
    "        output = self.get_k_hiddens(last_hiddens=hids, idx_list=top_k_idx, pooling=self.att_pooling) # self.get_k...\n",
    "        #print(output.shape)\n",
    "        output = self.fc1(output)\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n",
    "    def get_k_hiddens(self, last_hiddens, idx_list, pooling='mean'):\n",
    "        '''\n",
    "        @args\n",
    "        last_hiddens: bert last hidden states\n",
    "        idx_list: top_k_idxs\n",
    "        pooling: how get final rep. vectors, 'sum', 'mean', 'concat'\n",
    "        '''\n",
    "        final = list()\n",
    "        for idx, hid in zip(idx_list, last_hiddens):\n",
    "            if pooling=='sum':\n",
    "                final.append(torch.sum(hid[idx, :], dim=0).unsqueeze(0)) # (1, 768)\n",
    "            elif pooling=='mean' or pooling=='average':\n",
    "                final.append(torch.mean(hid[idx, :], dim=0).unsqueeze(0)) # (1, 768)\n",
    "            elif pooling=='concat':\n",
    "                final.append(hid[idx, :].view(1, -1)) # (1, 768*k)\n",
    "        final = torch.cat(final, dim=0) # to tensor (batch_size, 768) or (batch_size, 768*k) (concat)\n",
    "        return final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
